{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_fluency_v2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Hzt2mM-vZB_1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628220239342,"user_tz":-600,"elapsed":15478,"user":{"displayName":"Huan Koh","photoUrl":"","userId":"10520755133546248702"}},"outputId":"37b84c65-453f-43ec-e863-d5fc22f0fbae"},"source":["!pip install transformers &> /dev/null\n","!pip install datasets &> /dev/null\n","!pip install stanfordnlp &> /dev/null\n","import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":248},"id":"ZjvBtMW3eA0M","executionInfo":{"status":"error","timestamp":1628220788634,"user_tz":-600,"elapsed":436,"user":{"displayName":"Huan Koh","photoUrl":"","userId":"10520755133546248702"}},"outputId":"95d41261-ae32-44bd-d703-224542735884"},"source":["import stanfordnlp\n","stanford_nlp = stanfordnlp.Pipeline(processors='tokenize', lang='en')\n","\n","def stanford_tokenizer(text):\n","    doc = stanford_nlp(text)\n","    sentences = []\n","    for i, sentence in enumerate(doc.sentences):\n","        sent = [word.text for word in sentence.words]\n","        sentences.append(sent)\n","        \n","    return sentences"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Use device: gpu\n","---\n","Loading: tokenize\n","With settings: \n","{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n","Cannot load model from /root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt\n"],"name":"stdout"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"5XyPlhM9Z4sv"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"o8Fv0q78Z4Fc"},"source":["import torch\n","from torch.nn.functional import softmax\n","from transformers import BertForNextSentencePrediction, BertTokenizer\n","from transformers import AdamW\n","from transformers import get_scheduler\n","import nltk\n","from itertools import combinations, permutations\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n","import pprint\n","import statistics as stat\n","import numpy as np\n","import random\n","from collections import Counter\n","\n","\n","sent_detector = nltk.data.load('tokenizers/punkt/english.pickle') \n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FR7oeqz7e6MN"},"source":["\n","## Preparing Dataset for Fine-tuning the model"]},{"cell_type":"code","metadata":{"id":"tLjJ8mgXgF1f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628145410099,"user_tz":-600,"elapsed":23574,"user":{"displayName":"Huan Koh","photoUrl":"","userId":"10520755133546248702"}},"outputId":"b123b585-5d42-4e27-8703-b9685e221335"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","import json\n","path = 'drive/MyDrive/pubmed-dataset'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kPdg5odanRZ-"},"source":["file_dict ={}\n","path = 'drive/MyDrive/pubmed-dataset'\n","files = os.listdir(path)\n","files = [f for f in files if f != 'vocab']\n","#files = [f for f in files if 'test' in f]\n","# Import all train, val and test files\n","for fname in files:\n","    with open(os.path.join(path,fname),'r', encoding='utf-8') as f:\n","        file_dict[fname[:-4]] = [l.strip() for l in f]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"shlaoAozXbWN"},"source":["def process_sc_data(txt_file):\n","    '''\n","    Return a list of abstract sentence lists.\n","    '''\n","    master_abs_list = []\n","    master_full_list = []\n","    for paper in txt_file:\n","        abstract_list = [i.replace('<S>','').replace('</S>','').strip() for i in json.loads(paper)['abstract_text']]\n","        #abstract = ''.join(abstract).replace('<S>','').replace('</S>','').strip()\n","\n","        #full = ''.join(json.loads(paper)['article_text']).replace('\\n','').strip()\n","        full_list = json.loads(paper)['article_text']\n","\n","        # abstract list \n","        #abstract_list = sent_detector.tokenize(abstract)\n","        #full_list = sent_detector.tokenize(full)\n","        master_abs_list.append(abstract_list)\n","        master_full_list.append(full_list) \n","\n","    return master_abs_list,master_full_list\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcNQEEaOlEdZ"},"source":["def pre_sentpairs(abs,full):\n","    sampled_full = []\n","    assert len(abs) == len(full)\n","    # Length of abstract and full text\n","    doc_abs_sents = [len(i) for i in abs] \n","    doc_full_sents = [len(i) for i in full]\n","    # loop through all of them\n","    for i in range(len(abs)):\n","        # Length of abstract and doc\n","        abs_len = doc_abs_sents[i]\n","        full_len = doc_full_sents[i]\n","        to_sample = full_len-abs_len\n","        # \n","        if to_sample > 0:\n","            start = random.sample(range(to_sample),1)[0]\n","            end = start+abs_len\n","            sampled_full.append(full[i][start:end])\n","        elif len(full[i])>1:\n","            sampled_full.append(full[i])\n","        \n","    doc_sentences = abs+sampled_full\n","    \n","    return doc_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qIp6AAhj8RP"},"source":["def extract_neg_intersample(docs_sents_list,current_doc_index):\n","\n","    '''\n","    Given a doc_sents_list and an index, we extract one negative sample\n","    (i.e., a sentence) from other docs. \n","    '''\n","    wk_list = docs_sents_list.copy()\n","    # random sample one sent from current index\n","    intra_sent_index = random.sample(range(len(wk_list[current_doc_index])),1)[0]\n","    intra_sent = wk_list[current_doc_index][intra_sent_index]\n","    \n","    # random sample one index and ensure it is not the same index as current index\n","    inter_doc_index = random.sample(range(len(wk_list)),1)[0]\n","    while inter_doc_index == current_doc_index:\n","        inter_doc_index = random.sample(range(len(wk_list)),1)[0]\n","    inter_doc = wk_list[inter_doc_index]\n","    # Random select one sent_index\n","    inter_sent_index = random.sample(range(len(inter_doc)),1)[0]\n","    inter_sent = inter_doc[inter_sent_index]\n","\n","    info = {\n","        'sent1-index':intra_sent_index,\n","        'sent1-docindex':current_doc_index,\n","        'sent2-index': inter_sent_index,\n","        'sent2-docindex':inter_doc_index,\n","        'label':1}\n","\n","    return intra_sent, inter_sent, 1, info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEhcDiWzI0eq"},"source":["def process_sentpairs(docs_sents_list, interdoc_bias=0.5,negative_sample_amount = 1):\n","    '''\n","    Given a list of sentence list, find next sentence with label = 0 and \n","    create one negative samples of label = 1 (i.e., a balanced dataset).\n","    '''\n","    datasets = []\n","    for doc_index in range(len(docs_sents_list)):\n","        sents = docs_sents_list[doc_index]\n","        doc_length = len(sents)\n","        sent_pairs_index = list(permutations(range(doc_length),2))\n","        ##  Create data from same doc\n","        pos_samples = []\n","        neg_intrasamples = []\n","        for t in sent_pairs_index:\n","            if (t[0]+1) == t[1]:\n","                info = {'sent1-index':t[0],\n","                        'sent1-docindex':doc_index,\n","                        'sent2-index':t[1],\n","                        'sent2-docindex':doc_index,\n","                        'label':0}\n","                data = (sents[t[0]],sents[t[1]],0,info) # sent1, sent2, label, infos\n","                pos_samples.append(data)\n","            else:\n","                info = {'sent1-index':t[0],\n","                        'sent1-docindex':doc_index,\n","                        'sent2-index':t[1],\n","                        'sent2-docindex':doc_index,\n","                        'label':1}\n","                data = (sents[t[0]],sents[t[1]],1,info)\n","                neg_intrasamples.append(data)\n","        # Create negative samples from same doc and other docs\n","        desired_neg_amount = negative_sample_amount*len(pos_samples)\n","        neg_samples = []\n","        while len(neg_samples) < desired_neg_amount:\n","            # If larger than interdoc_bias then we sample from intradoc\n","            if random.uniform(0, 1) > interdoc_bias:\n","                random.shuffle(neg_intrasamples)\n","                intra_sample = neg_intrasamples.pop()\n","                neg_samples.append(intra_sample)\n","            # If smaller, we extract from interdocs (i.e., other docs)\n","            else:\n","                inter_sample = extract_neg_intersample(docs_sents_list,doc_index)\n","                neg_samples.append(inter_sample)\n","\n","        datasets += pos_samples\n","        datasets += neg_samples\n","    # Unzip and assign variables for readability and usability\n","    previous_sents, later_sents, labels, infos = list(map(list, zip(*datasets)))\n","\n","    return previous_sents, later_sents, labels, infos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOmpGLcVTaJe"},"source":["test_abs, test_full = process_sc_data(file_dict['test'])\n","test_data = process_sentpairs(pre_sentpairs(test_abs,test_full))\n","\n","val_abs, val_full = process_sc_data(file_dict['val'])\n","val_data = process_sentpairs(pre_sentpairs(val_abs,val_full))\n","\n","\n","train_abs, train_full = process_sc_data(file_dict['train'])\n","train_data = process_sentpairs(pre_sentpairs(train_abs,train_full))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvSHfLh9L9Fc"},"source":["class ScienceDataset(Dataset):\n","  def __init__(self, previous_sents, next_sents, labels, paper_index, tokenizer):\n","    self.previous_sents = previous_sents\n","    self.next_sents = next_sents\n","    self.labels = labels\n","    self.paper_index = paper_index\n","    self.tokenizer = tokenizer\n","  \n","  def __len__(self):\n","    return len(self.labels)\n","\n","  def __getitem__(self, idx):\n","    sent1 = self.previous_sents[idx]\n","    sent2 = self.next_sents[idx]\n","    label = self.labels[idx]\n","    index = self.paper_index[idx]\n","\n","    encoded = self.tokenizer.encode_plus(sent1, text_pair=sent2, is_split_into_words=False,\n","                   padding=True, truncation=True, return_tensors=\"pt\")\n","    length = encoded['input_ids'].shape[1]\n","    input_ids = encoded['input_ids'][0]\n","    token_type_ids = encoded['token_type_ids'][0]\n","    attention_mask = encoded['attention_mask'][0]\n","    encoded = dict(\n","        input_ids=input_ids,\n","        token_type_ids=token_type_ids,\n","        attention_mask=attention_mask\n","    )\n","    return encoded, label, length, index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBjT7wxeXpsx"},"source":["def collate_fn(data):\n","    \"\"\"\n","       data: is a list of tuples with (example, label, length)\n","             where 'example' is a tensor of arbitrary shape\n","             and label/length are scalars\n","             https://stackoverflow.com/questions/65279115/how-to-use-collate-fn-with-dataloaders\n","    \"\"\"\n","    _, labels, lengths, indexs = zip(*data)\n","    max_len = max(lengths)\n","    input_ids = torch.zeros((len(data), max_len))\n","    token_type_ids = torch.zeros((len(data), max_len))\n","    attention_mask = torch.zeros((len(data), max_len))\n","\n","    for i in range(len(data)):\n","      k = data[i][0]['input_ids'].size(0)\n","      input_ids[i] = torch.cat([data[i][0]['input_ids'], torch.zeros(max_len-k)])\n","      token_type_ids[i] = torch.cat([data[i][0]['token_type_ids'], torch.zeros(max_len-k)])\n","      attention_mask[i] = torch.cat([data[i][0]['attention_mask'], torch.zeros(max_len-k)])\n","\n","    encoded = dict(\n","        input_ids=input_ids.long(),\n","        token_type_ids=token_type_ids.long(),\n","        attention_mask=attention_mask.long()\n","    )\n","    return encoded, torch.Tensor(labels).long(), list(indexs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mzFFBYBMSOJD","executionInfo":{"status":"ok","timestamp":1628150065732,"user_tz":-600,"elapsed":16167,"user":{"displayName":"Huan Koh","photoUrl":"","userId":"10520755133546248702"}},"outputId":"fc398799-f9fc-4d46-8eed-b2ffd228fcdd"},"source":["# load pretrained model and a pretrained tokenizer\n","model = BertForNextSentencePrediction.from_pretrained('allenai/scibert_scivocab_uncased')\n","tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-E62HVqJTgaJ","executionInfo":{"status":"ok","timestamp":1628150066773,"user_tz":-600,"elapsed":3,"user":{"displayName":"Huan Koh","photoUrl":"","userId":"10520755133546248702"}},"outputId":"7f1ef421-20af-4845-d9e3-5b12bb44d053"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForNextSentencePrediction(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls): BertOnlyNSPHead(\n","    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"CYT9rLGKliIz"},"source":["# load pretrained model and a pretrained tokenizer\n","#model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n","#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NcGU8KV5UreF"},"source":["## train\n","train_sc = ScienceDataset(train_data[0],train_data[1],train_data[2],train_data[3],tokenizer)\n","train_loader = DataLoader(train_sc, batch_size=16, shuffle=True, collate_fn=collate_fn)\n","\n","\n","## val\n","val_sc = ScienceDataset(val_data[0],val_data[1],val_data[2],val_data[3],tokenizer)\n","val_loader = DataLoader(val_sc, batch_size=16, shuffle=True, collate_fn=collate_fn)\n","\n","## test\n","test_sc = ScienceDataset(test_data[0],test_data[1],test_data[2],test_data[3],tokenizer)\n","test_loader = DataLoader(test_sc, batch_size=16, shuffle=True, collate_fn=collate_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C2WOrGCThHGm"},"source":["### Codes for evaluating results"]},{"cell_type":"code","metadata":{"id":"cIgI28L-Kybo"},"source":["def evaluator(y_true, y_pred):\n","  accuracy = accuracy_score(y_true, y_pred)\n","  macro_f1 = f1_score(y_true, y_pred, average='macro')\n","  weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n","  return dict(\n","      accuracy=accuracy,\n","      macro_f1=macro_f1,\n","      weighted_f1=weighted_f1\n","  ) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xfg4e8ggXCAE"},"source":["def fluency(y_pred, y_keys):\n","  relevant_index = [(v['sent1-docindex'],k) for k,v in enumerate(y_keys) if v['label']==0]\n","  relevant_index_dict = {}\n","\n","  for paper_index, relevant_labelindex in relevant_index:\n","    relevant_index_dict.setdefault(paper_index,[]).append(relevant_labelindex)\n","  \n","  fluency_scores = {}\n","  for paper_index, rel_index in relevant_index_dict.items():\n","    if len(rel_index) > 0:\n","      #print([y_pred[i] for i in rel_index])\n","      #print(np.mean(np.array([y_pred[i] for i in rel_index])==0))\n","      fluency_scores[paper_index] = np.mean(np.array([y_pred[i] for i in rel_index])==0)\n","  \n","\n","  if len(fluency_scores) > 0:\n","    fluency_score = stat.mean(fluency_scores.values())\n","    return fluency_score\n","  else:\n","    return 'No relevant scores'\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e19hN3PhLQZH","outputId":"5eea35f8-75ec-4c78-d596-f5a92e0dcd00"},"source":["features, label, keys = next(iter(test_loader))\n","out = model(**features)[0]\n","pred = torch.softmax(out, dim=1)\n","\n","_, class_prediction = torch.max(pred, 1)\n","\n","print('Label', label)\n","print('Prediction',class_prediction)\n","print(evaluator(label, class_prediction))\n","print('Keys',keys)\n","print(fluency(class_prediction, keys))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"-y5O1AYpcGE6"},"source":["## Functions to evaluate data"]},{"cell_type":"code","metadata":{"id":"1zoN93rVlekz"},"source":["def eval_data(eval_loader):\n","    with torch.no_grad():\n","        val_label = []\n","        val_pred = []\n","        val_keys = []\n","        for data in tqdm(eval_loader):\n","            features, label, keys = data\n","            features = {k:v.to(device) for k,v in features.items()}\n","            label = label.to(device)\n","            outputs = model(**features)[0]\n","            pred = torch.softmax(outputs, dim=1)\n","            _, class_prediction = torch.max(pred, 1)\n","            val_label += label.tolist()\n","            val_pred += class_prediction.tolist()\n","            val_keys += keys\n","\n","    accuracy_f1 = evaluator(val_label,val_pred)\n","    eval_fluency = fluency(val_pred, val_keys)\n","    print('Prediction Counter:', Counter(val_pred))  \n","    print('Accuracy and F1-score: ', accuracy_f1)\n","    print('Fluency of validation dataset', eval_fluency)\n","\n","    return Counter(val_pred), accuracy_f1, eval_fluency\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DU2mLCqIldrk"},"source":["with open('drive/MyDrive/Summarization/PUBMED_SUMM/pegasus_pubmed.txt','r',encoding='utf-8') as f:\n","    pegasus = [sent_detector.tokenize(l.strip().replace('<n> ','')) for l in f]\n","    \n","with open('drive/MyDrive/Summarization/PUBMED_SUMM/bigbirdpegasus_pubmed.txt','r',encoding='utf-8') as f:\n","    bigbird = [sent_detector.tokenize(l.strip().replace('<n> ','')) for l in f]\n","    \n","with open('drive/MyDrive/Summarization/PUBMED_SUMM/hiporank_extsumm.txt','r',encoding='utf-8') as f:\n","    hiporank = [sent_detector.tokenize(l.strip().replace('<n> ','')) for l in f]\n","\n","pegasus_data = process_sentpairs(pegasus)\n","bigbird_data = process_sentpairs(bigbird)\n","hiporank_data = process_sentpairs(hiporank)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ohN6jewUcDNg"},"source":["\n","## Pegasus Loader\n","pegasus_sc = ScienceDataset(pegasus_data[0],pegasus_data[1],pegasus_data[2],pegasus_data[3],tokenizer)\n","pegasus_loader = DataLoader(pegasus_sc, batch_size=16, shuffle=True, collate_fn=collate_fn)\n","\n","## Bigbird Loader\n","bigbird_sc = ScienceDataset(bigbird_data[0],bigbird_data[1],bigbird_data[2],bigbird_data[3],tokenizer)\n","bigbird_loader = DataLoader(bigbird_sc, batch_size=16, shuffle=True, collate_fn=collate_fn)\n","\n","\n","## Hiporank Lodaer\n","hiporank_sc = ScienceDataset(hiporank_data[0],hiporank_data[1],hiporank_data[2],hiporank_data[3],tokenizer)\n","hiporank_loader = DataLoader(hiporank_sc, batch_size=16, shuffle=True, collate_fn=collate_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjpcpTjxgG85"},"source":["## Train Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kE4qLxkbm92r","executionInfo":{"status":"ok","timestamp":1626086777208,"user_tz":-600,"elapsed":25,"user":{"displayName":"Huan Koh","photoUrl":"","userId":"05002466660341682466"}},"outputId":"5e210c36-ba64-4a41-f165-8bef6f264ab4"},"source":["# Number of parameters\n","sum(p.numel() for p in model.parameters() if p.requires_grad)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["109920002"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"4wSX6En6aCvJ"},"source":["optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","num_epochs = 5\n","num_training_steps = num_epochs * len(train_loader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps\n",")\n","\n","criterion = torch.nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdDe1wUbdIzh"},"source":["model.to(device)\n","print('')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tal-SlVQf5Z5"},"source":["from tqdm.auto import tqdm\n","\n","progress_bar = tqdm(range(num_training_steps))\n","counter = 0 \n","total_loss = 0\n","\n","for epoch in range(num_epochs):\n","    print('EPOCH: ', epoch)\n","    # Evaluate model\n","    model.eval()\n","    print('Model performance on Validation Data Human Abstract')\n","    print('-'*80)\n","    eval_data(val_loader)\n","    print('Model performance on Test Data Human Abstract')\n","    print('-'*80)\n","    eval_data(test_loader)\n","    \n","    print('Model performance on Pegasus summary')\n","    print('-'*80)\n","    eval_data(pegasus_loader)\n","    print('Model performance on Big Bird summary')\n","    print('-'*80)\n","    eval_data(pegasus_loader) \n","    print('Model performance on HipoRank summary')\n","    print('-'*80)\n","    eval_data(hiporank_loader)\n","    \n","    \n","    master_label = []\n","    master_class_pred = []      \n","\n","    print(\"Start training for epoch:\", epoch)\n","    model.train()\n","    for batch in train_loader:\n","        features, label, _ = batch\n","        features = {k:v.to(device) for k,v in features.items()}\n","        label = label.to(device)\n","\n","        outputs = model(**features)[0]\n","        pred = torch.softmax(outputs, dim=1)\n","        loss = criterion(pred,label)\n","        loss.backward()\n","\n","        # Optimizer - trnainig step\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)\n","        # to print loss and prediction label\n","        # Class prediction\n","        _, class_prediction = torch.max(pred, 1)\n","        # Store in list\n","        master_label += label.tolist()\n","        master_class_pred += class_prediction.tolist()\n","        # Running loss\n","        total_loss += loss.item()\n","\n","        ## Show total loss and evaluation metric\n","        if counter%19999 == 0:\n","            print(total_loss)\n","            total_loss = 0\n","            print(evaluator(master_label, master_class_pred))\n","            master_label = []\n","            master_class_pred = []\n","        # counter\n","        counter +=1 \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vr6arRJTHI9B"},"source":[""],"execution_count":null,"outputs":[]}]}